{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "from collections import deque\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Paperspace/DinoRunTutorial/blob/master/Reinforcement%20Learning%20Dino%20Run.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_url = \"chrome://dino\"\n",
    "chrome_driver_path = ChromeDriverManager().install()\n",
    "\n",
    "loss_file_path = \"./objects/loss.csv\"\n",
    "actions_file_path = \"./objects/actions.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores.csv\"\n",
    "\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = process_img(screen)\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (80, 80))\n",
    "    return image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
    "        imS = cv2.resize(screen, (800, 400))\n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        service = Service(chrome_driver_path)\n",
    "        self._driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        self._driver.set_window_position(x=300,y=300)\n",
    "        self._driver.set_window_size(900, 600)\n",
    "        \n",
    "        try : \n",
    "            self._driver.get(game_url)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "        \n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element(\"tag name\", \"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element(\"tag name\", \"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self, game):\n",
    "        self._game = game\n",
    "        sleep(1)\n",
    "        self.jump()\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_state:\n",
    "    def __init__(self, agent, game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img()\n",
    "        self._display.__next__()\n",
    "        \n",
    "    def get_state(self, actions):\n",
    "        score = self._game.get_score()\n",
    "        reward = 1\n",
    "        is_over = False\n",
    "        \n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "            reward = -3\n",
    "        \n",
    "        image = grab_screen(self._game._driver)\n",
    "        self._display.send(image)\n",
    "        \n",
    "        if self._agent.is_crashed():\n",
    "            self._game.restart()\n",
    "            reward = -100\n",
    "            is_over = True\n",
    "        \n",
    "        return image, reward, is_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "PRETRAINED = False\n",
    "ACTIONS = 2\n",
    "GAMMA = 0.99\n",
    "OBSERVATION = 100.  # timesteps to observe before training\n",
    "EXPLORE = 100000.  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001  # final value of epsilon\n",
    "INITIAL_EPSILON = 0.01  # starting value of epsilon\n",
    "LEARNING_RATE = 1e-4\n",
    "REPLAY_MEMORY = 50000  # number of previous transitions to remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda :  True\n",
      "device :  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "print(\"cuda : \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device : \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DinoNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, (8, 8), stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, (4, 4), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, (3, 3), stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d((2, 2))\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, ACTIONS)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.max_pool2d(self.relu(self.conv1(x)))\n",
    "        x = self.max_pool2d(self.relu(self.conv2(x)))\n",
    "        x = self.max_pool2d(self.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DinoNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# create a new model if not exist\n",
    "if not os.path.isdir(\"./model\"):\n",
    "    os.makedirs(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model.load_state_dict(torch.load(f\"./latest.pth\"))\n",
    "    \n",
    "if PRETRAINED:\n",
    "    load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    for episode in range(num_episodes):\\n        total_loss = 0\\n        total_reward = 0\\n        game_state._game.restart()\\n\\n        # Get initial state\\n        state, reward, done = game_state.get_state([1, 0])  # Start with no action\\n        state = torch.tensor(state, device=device, dtype=torch.float).unsqueeze(0)\\n        \\n        # Reduce epsilon\\n        if episode % 1 == 0:\\n            epsilon = max(FINAL_EPSILON, epsilon - (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE)\\n\\n        while not game_state._agent.is_crashed():\\n            global_step += 1\\n            action = [0, 1] if random.random() <= epsilon else [1, 0]  # Random or best action based on epsilon\\n            next_state, reward, done = game_state.get_state(action)\\n            next_state = torch.tensor(next_state, device=device, dtype=torch.float).unsqueeze(0)\\n\\n            # Save transition to replay memory\\n            replay_memory.append((state, action, reward, next_state, done))\\n            state = next_state\\n            total_reward += reward\\n\\n            # Check if the memory is sufficient to sample from\\n            if len(replay_memory) >= batch_size:\\n                # Sample a minibatch from replay memory\\n                minibatch = random.sample(replay_memory, batch_size)\\n                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*minibatch)\\n\\n                state_batch = torch.stack(state_batch)\\n                next_state_batch = torch.stack(next_state_batch)\\n                reward_batch = torch.tensor(reward_batch, device=device, dtype=torch.float)\\n                action_batch = torch.tensor([a.index(1) for a in action_batch], device=device, dtype=torch.long).unsqueeze(1)\\n                done_batch = torch.tensor(done_batch, device=device, dtype=torch.float)\\n\\n                # Compute Q(s_t, a)\\n                current_q_values = model(state_batch).gather(1, action_batch)\\n\\n                # Compute Q(s_t+1) for all next states.\\n                next_q_values = model(next_state_batch).max(1)[0]\\n                # Compute the target Q values\\n                target_q_values = reward_batch + (GAMMA * next_q_values * (1 - done_batch))\\n\\n                # Compute Bellman error\\n                loss = loss_fn(current_q_values.squeeze(1), target_q_values.detach())\\n\\n                # Optimize the model\\n                optimizer.zero_grad()\\n                loss.backward()\\n                optimizer.step()\\n\\n                total_loss += loss.item()\\n                print(f\"g_step: {global_step}, action: {np.argmax(action)}, reward: {reward}, loss: {loss.item()}\")\\n\\n            sleep(0.05)  # Sleep to decrease the speed of execution\\n\\n        print(f\"Episode {episode + 1}, Total reward: {total_reward}, Total loss: {total_loss}, Epsilon: {epsilon}\")\\n\\n        # Optionally save the model\\n        if episode % 10 == 0:\\n            torch.save(model.state_dict(), f\"./model/dino_net_{episode}.pth\")\\n\\n    # Save the final model\\n    torch.save(model.state_dict(), \"./model/dino_net_final.pth\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainNetwork(model, game_state, optimizer, loss_fn, num_episodes, batch_size=32):\n",
    "    replay_memory = deque(maxlen=REPLAY_MEMORY)\n",
    "    D = []\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    \n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    print(f's_t shape : {s_t.shape}')\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])\n",
    "    initial_state = s_t\n",
    "    \n",
    "    while(True):\n",
    "        loss_sum = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        random_action = 0\n",
    "        r_t = 0\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        \n",
    "        if random.random() <= epsilon:\n",
    "            random_action = 1\n",
    "            action_index = random.randrange(ACTIONS)\n",
    "            a_t[action_index] = 1\n",
    "        else:\n",
    "            q = model(torch.tensor(s_t, device=device, dtype=torch.float))\n",
    "            max_Q, action_index = torch.max(q, 1)\n",
    "            a_t[action_index] = 1\n",
    "        \n",
    "        # reduce epsilon\n",
    "        if epsilon > FINAL_EPSILON:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "        \n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1)\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "        \n",
    "        D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "        \n",
    "        if len(D) > OBSERVATION:\n",
    "            minibatch = random.sample(D, batch_size)\n",
    "            inputs = np.zeros((batch_size, s_t.shape[1], s_t.shape[2], s_t.shape[3]))\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                state_t = minibatch[i][0]\n",
    "                action_t = minibatch[i][1]\n",
    "                reward_t = minibatch[i][2]\n",
    "                state_t1 = minibatch[i][3]\n",
    "                terminal = minibatch[i][4]\n",
    "                \n",
    "                inputs[i:i+1] = state_t\n",
    "                targets[i] = model(torch.tensor(state_t, device=device, dtype=torch.float)).cpu().detach().numpy()\n",
    "                Q_sa = model(torch.tensor(state_t1, device=device, dtype=torch.float)).cpu().detach().numpy()\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, np.argmax(action_t)] = reward_t\n",
    "                else:\n",
    "                    targets[i, np.argmax(action_t)] = reward_t + GAMMA * np.max(Q_sa)\n",
    "            \n",
    "                outputs = model(torch.tensor(inputs, device=device, dtype=torch.float))\n",
    "                loss = loss_fn(outputs, torch.tensor(targets, device=device, dtype=torch.float))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_sum += loss.item()\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "    for episode in range(num_episodes):\n",
    "        total_loss = 0\n",
    "        total_reward = 0\n",
    "        game_state._game.restart()\n",
    "\n",
    "        # Get initial state\n",
    "        state, reward, done = game_state.get_state([1, 0])  # Start with no action\n",
    "        state = torch.tensor(state, device=device, dtype=torch.float).unsqueeze(0)\n",
    "        \n",
    "        # Reduce epsilon\n",
    "        if episode % 1 == 0:\n",
    "            epsilon = max(FINAL_EPSILON, epsilon - (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE)\n",
    "\n",
    "        while not game_state._agent.is_crashed():\n",
    "            global_step += 1\n",
    "            action = [0, 1] if random.random() <= epsilon else [1, 0]  # Random or best action based on epsilon\n",
    "            next_state, reward, done = game_state.get_state(action)\n",
    "            next_state = torch.tensor(next_state, device=device, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Check if the memory is sufficient to sample from\n",
    "            if len(replay_memory) >= batch_size:\n",
    "                # Sample a minibatch from replay memory\n",
    "                minibatch = random.sample(replay_memory, batch_size)\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*minibatch)\n",
    "\n",
    "                state_batch = torch.stack(state_batch)\n",
    "                next_state_batch = torch.stack(next_state_batch)\n",
    "                reward_batch = torch.tensor(reward_batch, device=device, dtype=torch.float)\n",
    "                action_batch = torch.tensor([a.index(1) for a in action_batch], device=device, dtype=torch.long).unsqueeze(1)\n",
    "                done_batch = torch.tensor(done_batch, device=device, dtype=torch.float)\n",
    "\n",
    "                # Compute Q(s_t, a)\n",
    "                current_q_values = model(state_batch).gather(1, action_batch)\n",
    "\n",
    "                # Compute Q(s_t+1) for all next states.\n",
    "                next_q_values = model(next_state_batch).max(1)[0]\n",
    "                # Compute the target Q values\n",
    "                target_q_values = reward_batch + (GAMMA * next_q_values * (1 - done_batch))\n",
    "\n",
    "                # Compute Bellman error\n",
    "                loss = loss_fn(current_q_values.squeeze(1), target_q_values.detach())\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                print(f\"g_step: {global_step}, action: {np.argmax(action)}, reward: {reward}, loss: {loss.item()}\")\n",
    "\n",
    "            sleep(0.05)  # Sleep to decrease the speed of execution\n",
    "\n",
    "        print(f\"Episode {episode + 1}, Total reward: {total_reward}, Total loss: {total_loss}, Epsilon: {epsilon}\")\n",
    "\n",
    "        # Optionally save the model\n",
    "        if episode % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"./model/dino_net_{episode}.pth\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"./model/dino_net_final.pth\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame():\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_state(dino, game)\n",
    "    try :\n",
    "        trainNetwork(model, game_state, optimizer, loss_fn, num_episodes=1000)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_t shape : (80, 80, 4)\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=124.0.6367.63)\nStacktrace:\n\tGetHandleVerifier [0x0089C113+48259]\n\t(No symbol) [0x0082CA41]\n\t(No symbol) [0x00720A17]\n\t(No symbol) [0x006FE02B]\n\t(No symbol) [0x0078742E]\n\t(No symbol) [0x00799476]\n\t(No symbol) [0x00780B36]\n\t(No symbol) [0x0075570D]\n\t(No symbol) [0x007562CD]\n\tGetHandleVerifier [0x00B56533+2908323]\n\tGetHandleVerifier [0x00B93B4B+3159739]\n\tGetHandleVerifier [0x0093505B+674763]\n\tGetHandleVerifier [0x0093B21C+699788]\n\t(No symbol) [0x00836244]\n\t(No symbol) [0x00832298]\n\t(No symbol) [0x0083242C]\n\t(No symbol) [0x00824BB0]\n\tBaseThreadInitThunk [0x755B7BA9+25]\n\tRtlInitializeExceptionChain [0x7712BE3B+107]\n\tRtlClearBits [0x7712BDBF+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplayGame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mplayGame\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m game_state \u001b[38;5;241m=\u001b[39m Game_state(dino, game)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m :\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtrainNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     game\u001b[38;5;241m.\u001b[39mend()\n",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m, in \u001b[0;36mtrainNetwork\u001b[1;34m(model, game_state, optimizer, loss_fn, num_episodes, batch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epsilon \u001b[38;5;241m>\u001b[39m FINAL_EPSILON:\n\u001b[0;32m     35\u001b[0m     epsilon \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m (INITIAL_EPSILON \u001b[38;5;241m-\u001b[39m FINAL_EPSILON) \u001b[38;5;241m/\u001b[39m EXPLORE\n\u001b[1;32m---> 37\u001b[0m x_t1, r_t, terminal \u001b[38;5;241m=\u001b[39m \u001b[43mgame_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m x_t1 \u001b[38;5;241m=\u001b[39m x_t1\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, x_t1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x_t1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m s_t1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(x_t1, s_t[:, :, :, :\u001b[38;5;241m3\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mGame_state.get_state\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[1;32m----> 9\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_game\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m     is_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mGame.get_score\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_score\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     score_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_script\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn Runner.instance_.distanceMeter.digits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(score_array)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(score)\n",
      "File \u001b[1;32md:\\Programming\\DinoGameAI\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:407\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    404\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[0;32m    405\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscript\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_args\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Programming\\DinoGameAI\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Programming\\DinoGameAI\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=124.0.6367.63)\nStacktrace:\n\tGetHandleVerifier [0x0089C113+48259]\n\t(No symbol) [0x0082CA41]\n\t(No symbol) [0x00720A17]\n\t(No symbol) [0x006FE02B]\n\t(No symbol) [0x0078742E]\n\t(No symbol) [0x00799476]\n\t(No symbol) [0x00780B36]\n\t(No symbol) [0x0075570D]\n\t(No symbol) [0x007562CD]\n\tGetHandleVerifier [0x00B56533+2908323]\n\tGetHandleVerifier [0x00B93B4B+3159739]\n\tGetHandleVerifier [0x0093505B+674763]\n\tGetHandleVerifier [0x0093B21C+699788]\n\t(No symbol) [0x00836244]\n\t(No symbol) [0x00832298]\n\t(No symbol) [0x0083242C]\n\t(No symbol) [0x00824BB0]\n\tBaseThreadInitThunk [0x755B7BA9+25]\n\tRtlInitializeExceptionChain [0x7712BE3B+107]\n\tRtlClearBits [0x7712BDBF+191]\n"
     ]
    }
   ],
   "source": [
    "playGame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
