{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from collections import deque\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Paperspace/DinoRunTutorial/blob/master/Reinforcement%20Learning%20Dino%20Run.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_url = \"chrome://dino\"\n",
    "chrome_driver_path = ChromeDriverManager().install()\n",
    "\n",
    "loss_file_path = \"./objects/loss.csv\"\n",
    "actions_file_path = \"./objects/actions.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores.csv\"\n",
    "\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open('objects/' + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = process_img(screen)\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image[:300, :500] # Crop Region of Interest (ROI)\n",
    "    image = cv2.resize(image, (80, 80))\n",
    "    return image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
    "        imS = cv2.resize(screen, (800, 400))\n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "def obj_init(D, t, epsilon):    \n",
    "    if not os.path.isdir('objects'):\n",
    "        os.makedirs('objects')\n",
    "    \n",
    "    if not os.path.isfile('./objects/D.pkl'):\n",
    "        save_obj(D, \"D\")\n",
    "    if not os.path.isfile('./objects/time.pkl'):\n",
    "        save_obj(t, \"time\")\n",
    "    if not os.path.isfile('./objects/epsilon.pkl'):\n",
    "        save_obj(epsilon, \"epsilon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        service = Service(chrome_driver_path)\n",
    "        self._driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        self._driver.set_window_position(x=300,y=300)\n",
    "        self._driver.set_window_size(900, 600)\n",
    "        \n",
    "        try : \n",
    "            self._driver.get(game_url)\n",
    "        except:\n",
    "            pass\n",
    "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element(\"tag name\", \"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element(\"tag name\", \"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self, game):\n",
    "        self._game = game\n",
    "        self.jump()\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_state:\n",
    "    def __init__(self, agent, game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img()\n",
    "        self._display.__next__()\n",
    "        \n",
    "    def get_state(self, actions):\n",
    "        actions_df.loc[len(actions_df)] = [actions]\n",
    "        score = self._game.get_score()\n",
    "        reward = 0.1\n",
    "        is_over = False\n",
    "        \n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "            reward = -0.01\n",
    "        \n",
    "        image = grab_screen(self._game._driver)\n",
    "        self._display.send(image)\n",
    "        \n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(loss_df)] = score\n",
    "            self._game.restart()\n",
    "            reward = -10\n",
    "            is_over = True\n",
    "        \n",
    "        return image, reward, is_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(scores_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df = pd.read_csv(q_value_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "ACTIONS = 2\n",
    "GAMMA = 0.99\n",
    "OBSERBATION = 100.  # timesteps to observe before training\n",
    "EXPLORE = 100000.  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001  # final value of epsilon\n",
    "INITIAL_EPSILON = 0.01  # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000  # number of previous transitions to remember\n",
    "BATCH_SIZE = 16  # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_channels = 4  # We stack 4 frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DinoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DinoNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, (8, 8), stride = 4, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, (4, 4), stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, (3, 3), stride = 1, padding = 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d((2, 2))\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, ACTIONS)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.max_pool2d(self.relu(self.conv1(x)))\n",
    "        x = self.max_pool2d(self.relu(self.conv2(x)))\n",
    "        x = self.max_pool2d(self.relu(self.conv3(x)))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DinoNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# create a new model if not exist\n",
    "if not os.path.isdir(\"./model\"):\n",
    "    os.makedirs(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model.load_state_dict(torch.load(f\"./latest.pth\"))\n",
    "    \n",
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(model, game_state, observe = False):\n",
    "    last_time = time.time()\n",
    "    D = load_obj(\"D\")       # Load from file\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1       # 0 => do nothing, 1 => jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2]) # 1*20*40*4\n",
    "    initial_state = s_t\n",
    "    \n",
    "    if observe:\n",
    "        OBSERVE = 999999999    # We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "    \n",
    "    else:\n",
    "        OBSERVE = OBSERBATION\n",
    "        epsilon = load_obj(\"epsilon\")\n",
    "    \n",
    "    t = load_obj(\"time\")        # resume from the previous time step\n",
    "    while(True):\n",
    "        loss_sum = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        random_action = 0\n",
    "        r_t = 0      # reward at 4\n",
    "        a_t = np.zeros([ACTIONS])   # action at t\n",
    "        \n",
    "        # choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "            if random.random() <= epsilon:\n",
    "                random_action = 1\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else:\n",
    "                q = model(torch.tensor(s_t).float())\n",
    "                max_Q, action_index = torch.max(q, 1)\n",
    "                a_t[action_index] = 1\n",
    "                \n",
    "        # reduce epsilon gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "            \n",
    "            \n",
    "        # run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1)\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "        \n",
    "        # store the transition in D\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.pop()\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        \n",
    "        # only train if done observing\n",
    "        if t > OBSERBATION:\n",
    "            minibatch = random.sample(D, BATCH_SIZE)\n",
    "            inputs = np.zeros((BATCH_SIZE, s_t.shape[1], s_t.shape[2], s_t.shape[3]))\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))\n",
    "            \n",
    "            for i in range(BATCH_SIZE):\n",
    "                state_t = minibatch[i][0]   # 4D stack of images\n",
    "                action_t = minibatch[i][1]  # Action index\n",
    "                reward_t = minibatch[i][2]  # reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]  # next state\n",
    "                terminal = minibatch[i][4]  # wheather the agent died or survided due the action\n",
    "                \n",
    "                inputs[i:i+1] = state_t\n",
    "                targets[i] = model(torch.tensor(state_t).float()).detach().numpy()\n",
    "                Q_sa = model(torch.tensor(state_t1).float()).detach().numpy()\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            # train\n",
    "            outputs = model(torch.tensor(inputs).float())\n",
    "            loss = loss_fn(outputs, torch.tensor(targets).float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "            loss_df.loc[len(loss_df)] = loss_sum\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "    \n",
    "        s_t = initial_state if terminal else s_t1   # reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            game_state._game.pause() #pause game while saving to filesystem\n",
    "            torch.save(model.state_dict(), f\"./model/episode_{t}.pth\")\n",
    "            torch.save(model.state_dict(), f\"./latest.pth\")\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            game_state._game.resume()\n",
    "            \n",
    "        print(f'timestep: {t}, random: {random_action}, epsilon: {round(epsilon, 3)}, action: {action_index}, reward: {r_t}, Q_max: {round(np.max(Q_sa),3)}, loss: {round(loss_sum, 3)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame(observe=False):\n",
    "    # obj_init([], 0, INITIAL_EPSILON)\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_state(dino, game)\n",
    "    try :\n",
    "        trainNetwork(model, game_state, observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 22001, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -0.9639999866485596, loss: 0.411\n",
      "timestep: 22002, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.840999960899353, loss: 0.489\n",
      "timestep: 22003, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 0.10300000011920929, loss: 0.999\n",
      "timestep: 22004, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 1.5130000114440918, loss: 0.496\n",
      "timestep: 22005, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.3910000324249268, loss: 0.623\n",
      "timestep: 22006, random: 0, epsilon: 0.008, action: tensor([1]), reward: -10, Q_max: 1.809999942779541, loss: 0.413\n",
      "timestep: 22007, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -3.002000093460083, loss: 1.742\n",
      "timestep: 22008, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -3.747999906539917, loss: 5.355\n",
      "timestep: 22009, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.5009999871253967, loss: 0.636\n",
      "timestep: 22010, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.7430000305175781, loss: 0.456\n",
      "timestep: 22011, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.1929999589920044, loss: 2.525\n",
      "timestep: 22012, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.9869999885559082, loss: 1.186\n",
      "timestep: 22013, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 1.9199999570846558, loss: 0.828\n",
      "timestep: 22014, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -0.06300000101327896, loss: 0.398\n",
      "timestep: 22015, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.0729999542236328, loss: 0.233\n",
      "timestep: 22016, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -1.2259999513626099, loss: 1.096\n",
      "timestep: 22017, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 1.0570000410079956, loss: 0.104\n",
      "timestep: 22018, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -1.3849999904632568, loss: 1.414\n",
      "timestep: 22019, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 1.034999966621399, loss: 0.543\n",
      "timestep: 22020, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 2.934000015258789, loss: 1.296\n",
      "timestep: 22021, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 2.48799991607666, loss: 0.075\n",
      "timestep: 22022, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.7569999694824219, loss: 2.79\n",
      "timestep: 22023, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -4.3470001220703125, loss: 1.259\n",
      "timestep: 22024, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 3.312000036239624, loss: 4.118\n",
      "timestep: 22025, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 0.5289999842643738, loss: 1.606\n",
      "timestep: 22026, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 1.4630000591278076, loss: 0.158\n",
      "timestep: 22027, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.559999942779541, loss: 0.351\n",
      "timestep: 22028, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 2.6089999675750732, loss: 2.135\n",
      "timestep: 22029, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 2.0350000858306885, loss: 0.229\n",
      "timestep: 22030, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.6669999957084656, loss: 0.391\n",
      "timestep: 22031, random: 0, epsilon: 0.008, action: tensor([1]), reward: -10, Q_max: 1.6820000410079956, loss: 0.368\n",
      "timestep: 22032, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.2690000534057617, loss: 2.008\n",
      "timestep: 22033, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 0.8610000014305115, loss: 3.159\n",
      "timestep: 22034, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 1.8589999675750732, loss: 1.039\n",
      "timestep: 22035, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -3.2920000553131104, loss: 0.945\n",
      "timestep: 22036, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -9.604000091552734, loss: 0.524\n",
      "timestep: 22037, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.1699999570846558, loss: 0.661\n",
      "timestep: 22038, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.2029999941587448, loss: 0.507\n",
      "timestep: 22039, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -2.7990000247955322, loss: 0.624\n",
      "timestep: 22040, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.2350000143051147, loss: 0.363\n",
      "timestep: 22041, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.2669999599456787, loss: 0.385\n",
      "timestep: 22042, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -1.1269999742507935, loss: 0.833\n",
      "timestep: 22043, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 2.2019999027252197, loss: 0.716\n",
      "timestep: 22044, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.1410000324249268, loss: 0.784\n",
      "timestep: 22045, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 0.017999999225139618, loss: 0.628\n",
      "timestep: 22046, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.0430000014603138, loss: 1.071\n",
      "timestep: 22047, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 0.8199999928474426, loss: 0.582\n",
      "timestep: 22048, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 2.2100000381469727, loss: 0.718\n",
      "timestep: 22049, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -4.2779998779296875, loss: 2.709\n",
      "timestep: 22050, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.6269999742507935, loss: 0.577\n",
      "timestep: 22051, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.14399999380111694, loss: 0.966\n",
      "timestep: 22052, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -5.566999912261963, loss: 1.978\n",
      "timestep: 22053, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -2.546999931335449, loss: 0.441\n",
      "timestep: 22054, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.1459999978542328, loss: 0.426\n",
      "timestep: 22055, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -4.6539998054504395, loss: 0.313\n",
      "timestep: 22056, random: 0, epsilon: 0.008, action: tensor([1]), reward: -10, Q_max: 1.3339999914169312, loss: 0.438\n",
      "timestep: 22057, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -0.1469999998807907, loss: 0.455\n",
      "timestep: 22058, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 0.2290000021457672, loss: 2.166\n",
      "timestep: 22059, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.0720000267028809, loss: 3.739\n",
      "timestep: 22060, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.9300000071525574, loss: 3.106\n",
      "timestep: 22061, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 0.7950000166893005, loss: 0.754\n",
      "timestep: 22062, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 0.10400000214576721, loss: 0.95\n",
      "timestep: 22063, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -0.33399999141693115, loss: 1.315\n",
      "timestep: 22064, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -3.993000030517578, loss: 0.714\n",
      "timestep: 22065, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.41600000858306885, loss: 1.211\n",
      "timestep: 22066, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -1.9140000343322754, loss: 0.79\n",
      "timestep: 22067, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 0.7099999785423279, loss: 1.072\n",
      "timestep: 22068, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -2.0989999771118164, loss: 0.676\n",
      "timestep: 22069, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -4.965000152587891, loss: 0.952\n",
      "timestep: 22070, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -5.883999824523926, loss: 0.985\n",
      "timestep: 22071, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -5.011000156402588, loss: 1.514\n",
      "timestep: 22072, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -1.8739999532699585, loss: 1.052\n",
      "timestep: 22073, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -4.639999866485596, loss: 0.75\n",
      "timestep: 22074, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -4.85099983215332, loss: 1.675\n",
      "timestep: 22075, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.04500000178813934, loss: 0.399\n",
      "timestep: 22076, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -2.740999937057495, loss: 0.745\n",
      "timestep: 22077, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 0.9150000214576721, loss: 0.623\n",
      "timestep: 22078, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -2.990999937057495, loss: 1.102\n",
      "timestep: 22079, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 0.07999999821186066, loss: 0.766\n",
      "timestep: 22080, random: 0, epsilon: 0.008, action: tensor([1]), reward: -10, Q_max: -1.7599999904632568, loss: 1.193\n",
      "timestep: 22081, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -4.019000053405762, loss: 0.875\n",
      "timestep: 22082, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -10.003999710083008, loss: 5.482\n",
      "timestep: 22083, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -5.485000133514404, loss: 0.905\n",
      "timestep: 22084, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.7210000157356262, loss: 0.437\n",
      "timestep: 22085, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -6.202000141143799, loss: 0.952\n",
      "timestep: 22086, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.13899999856948853, loss: 0.595\n",
      "timestep: 22087, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.5839999914169312, loss: 0.753\n",
      "timestep: 22088, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -5.252999782562256, loss: 0.3\n",
      "timestep: 22089, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -1.8730000257492065, loss: 2.409\n",
      "timestep: 22090, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -1.9809999465942383, loss: 0.416\n",
      "timestep: 22091, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -2.611999988555908, loss: 1.215\n",
      "timestep: 22092, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.004000000189989805, loss: 1.884\n",
      "timestep: 22093, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.4090000092983246, loss: 0.836\n",
      "timestep: 22094, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -2.309999942779541, loss: 0.744\n",
      "timestep: 22095, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -2.312000036239624, loss: 0.939\n",
      "timestep: 22096, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -8.506999969482422, loss: 0.504\n",
      "timestep: 22097, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 0.09200000017881393, loss: 0.383\n",
      "timestep: 22098, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -1.8580000400543213, loss: 0.452\n",
      "timestep: 22099, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 0.7710000276565552, loss: 0.683\n",
      "timestep: 22100, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -2.5220000743865967, loss: 0.672\n",
      "timestep: 22101, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -1.3450000286102295, loss: 0.585\n",
      "timestep: 22102, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -7.943999767303467, loss: 1.264\n",
      "timestep: 22103, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -1.4049999713897705, loss: 0.579\n",
      "timestep: 22104, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -0.7210000157356262, loss: 1.972\n",
      "timestep: 22105, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 1.0720000267028809, loss: 0.985\n",
      "timestep: 22106, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: -7.919000148773193, loss: 0.697\n",
      "timestep: 22107, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.8349999785423279, loss: 0.337\n",
      "timestep: 22108, random: 0, epsilon: 0.008, action: tensor([1]), reward: -10, Q_max: -3.062000036239624, loss: 0.372\n",
      "timestep: 22109, random: 0, epsilon: 0.008, action: tensor([1]), reward: -0.01, Q_max: 0.2540000081062317, loss: 0.426\n",
      "timestep: 22110, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.15199999511241913, loss: 0.363\n",
      "timestep: 22111, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -2.305999994277954, loss: 0.46\n",
      "timestep: 22112, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.23399999737739563, loss: 0.395\n",
      "timestep: 22113, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -4.301000118255615, loss: 0.52\n",
      "timestep: 22114, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -1.9620000123977661, loss: 0.258\n",
      "timestep: 22115, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 0.4909999966621399, loss: 0.305\n",
      "timestep: 22116, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 0.6259999871253967, loss: 0.695\n",
      "timestep: 22117, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: 0.5389999747276306, loss: 0.293\n",
      "timestep: 22118, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -1.305999994277954, loss: 2.171\n",
      "timestep: 22119, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -4.254000186920166, loss: 0.206\n",
      "timestep: 22120, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -1.8589999675750732, loss: 0.411\n",
      "timestep: 22121, random: 0, epsilon: 0.008, action: tensor([0]), reward: 0.1, Q_max: -0.9240000247955322, loss: 2.795\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=123.0.6312.106)\nStacktrace:\n\tGetHandleVerifier [0x00A84CE3+225091]\n\t(No symbol) [0x009B4E31]\n\t(No symbol) [0x00859A7A]\n\t(No symbol) [0x0083E312]\n\t(No symbol) [0x008B517B]\n\t(No symbol) [0x008C55A6]\n\t(No symbol) [0x008AF2F6]\n\t(No symbol) [0x008879B9]\n\t(No symbol) [0x0088879D]\n\tsqlite3_dbdata_init [0x00EF9A83+4064547]\n\tsqlite3_dbdata_init [0x00F0108A+4094762]\n\tsqlite3_dbdata_init [0x00EFB988+4072488]\n\tsqlite3_dbdata_init [0x00BFC9E9+930953]\n\t(No symbol) [0x009C0804]\n\t(No symbol) [0x009BAD28]\n\t(No symbol) [0x009BAE51]\n\t(No symbol) [0x009ACAC0]\n\tBaseThreadInitThunk [0x75DE7BA9+25]\n\tRtlInitializeExceptionChain [0x776FBD3B+107]\n\tRtlClearBits [0x776FBCBF+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplayGame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobserve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m, in \u001b[0;36mplayGame\u001b[1;34m(observe)\u001b[0m\n\u001b[0;32m      5\u001b[0m game_state \u001b[38;5;241m=\u001b[39m Game_state(dino, game)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m :\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtrainNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     game\u001b[38;5;241m.\u001b[39mend()\n",
      "Cell \u001b[1;32mIn[28], line 46\u001b[0m, in \u001b[0;36mtrainNetwork\u001b[1;34m(model, game_state, observe)\u001b[0m\n\u001b[0;32m     42\u001b[0m     epsilon \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m (INITIAL_EPSILON \u001b[38;5;241m-\u001b[39m FINAL_EPSILON) \u001b[38;5;241m/\u001b[39m EXPLORE\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# run the selected action and observed next state and reward\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m x_t1, r_t, terminal \u001b[38;5;241m=\u001b[39m \u001b[43mgame_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m last_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     48\u001b[0m x_t1 \u001b[38;5;241m=\u001b[39m x_t1\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, x_t1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x_t1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m, in \u001b[0;36mGame_state.get_state\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[0;32m      9\u001b[0m     actions_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(actions_df)] \u001b[38;5;241m=\u001b[39m [actions]\n\u001b[1;32m---> 10\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_game\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     12\u001b[0m     is_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 28\u001b[0m, in \u001b[0;36mGame.get_score\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_score\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 28\u001b[0m     score_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_script\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn Runner.instance_.distanceMeter.digits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(score_array)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(score)\n",
      "File \u001b[1;32md:\\Programming\\DinoGameAI\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:407\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    404\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[0;32m    405\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscript\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_args\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Programming\\DinoGameAI\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Programming\\DinoGameAI\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=123.0.6312.106)\nStacktrace:\n\tGetHandleVerifier [0x00A84CE3+225091]\n\t(No symbol) [0x009B4E31]\n\t(No symbol) [0x00859A7A]\n\t(No symbol) [0x0083E312]\n\t(No symbol) [0x008B517B]\n\t(No symbol) [0x008C55A6]\n\t(No symbol) [0x008AF2F6]\n\t(No symbol) [0x008879B9]\n\t(No symbol) [0x0088879D]\n\tsqlite3_dbdata_init [0x00EF9A83+4064547]\n\tsqlite3_dbdata_init [0x00F0108A+4094762]\n\tsqlite3_dbdata_init [0x00EFB988+4072488]\n\tsqlite3_dbdata_init [0x00BFC9E9+930953]\n\t(No symbol) [0x009C0804]\n\t(No symbol) [0x009BAD28]\n\t(No symbol) [0x009BAE51]\n\t(No symbol) [0x009ACAC0]\n\tBaseThreadInitThunk [0x75DE7BA9+25]\n\tRtlInitializeExceptionChain [0x776FBD3B+107]\n\tRtlClearBits [0x776FBCBF+191]\n"
     ]
    }
   ],
   "source": [
    "playGame(observe=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
